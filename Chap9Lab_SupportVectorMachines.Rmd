---
title: "Chapter 9 Support Vector Machines"
author: "CAA"
date: "July 24, 2015"
output: html_document
---

R Computer Lab for Chapter 9 Support Vector Machines, from the book Introduction to Statistical Learning


# Support Vector Classifier

### Generate observations for two classes:
```{r}
set.seed(1)
x = matrix(rnorm(20*2),ncol=2)
y = c(rep(-1,10), rep(1,10))
x[y==1,] = x[y==1,] + 1
```


### Are the classes linearly separable?
```{r}
par(mfrow=c(1,1))
plot(x, pch=16, col=(3-y))
```

They aren't.


### Fit support vector classifier.
```{r}
dat = data.frame(x=x, y=as.factor(y)) # need factors for classification
library(e1071)
svmfit = svm(y~., data=dat, kernel="linear", cost=10, scale=FALSE)
```

### Plot the support vector classifier:
```{r}
par(mfrow=c(1,1))
plot(svmfit, dat)
```


### Support vector identity:
```{r}
svmfit$index
```


### Basic information:
```{r}
summary(svmfit)
```


### What about a smaller cost parameter?
```{r}
svmfit = svm(y~.,data=dat,kernel="linear",cost=0.1,scale=FALSE)
par(mfrow=c(1,1))
plot(svmfit, dat)
svmfit$index
```


### Perform cross-validation using a range of cost values:
```{r}
set.seed(1)
tune.out = tune(svm, y~.,data=dat,kernel="linear",
   ranges=list(cost=c(0.001,0.01,0.1,1,5,10,100)))
summary(tune.out)
```


### Access the best model:
```{r}
bestmod = tune.out$best.model
summary(bestmod)
```



### Predict on a test set:
```{r}
xtest = matrix(rnorm(20*2), ncol=2)
ytest = sample(c(-1,1), 20, rep=TRUE)
xtest[ytest==1,] = xtest[ytest==1,] + 1
testdat = data.frame(x=xtest,y=as.factor(ytest))
```


### Make predictions using the best model:
```{r}
ypred = predict(bestmod,testdat)
table(predict=ypred, truth=testdat$y)
```


### What if we used a cost of 0.01?
```{r}
svmfit = svm(y~., data=dat, kernel="linear", cost=0.01,
             scale=FALSE)
ypred = predict(svmfit, testdat)
table(predict=ypred, truth=testdat$y)
```


### Consider when two classes are linearly separable:
```{r}
x[y==1,] = x[y==1,] + 0.5
par(mfrow=c(1,1))
plot(x, col=(y+5)/2, pch=19)
```



### Fit with large cost so no misclassifications:
```{r}
dat = data.frame(x=x,y=as.factor(y))
svmfit = svm(y~.,data=dat, kernel="linear", cost=1e5)
summary(svmfit)
par(mfrow=c(1,1))
plot(svmfit, dat)
```


### Try with smaller cost:
```{r}
svmfit = svm(y~.,data=dat, kernel="linear", cost=1)
summary(svmfit)
par(mfrow=c(1,1))
plot(svmfit,dat)
```


# Support Vector Machine

### Generate data with non-linear class boundary:
```{r}
set.seed(1)
x = matrix(rnorm(200*2), ncol=2)
x[1:100,] = x[1:100,] + 2
x[101:150,] = x[101:150,] - 2
y = c(rep(1,150),rep(2,50))
dat = data.frame(x=x, y=as.factor(y))
par(mfrow=c(1,1))
plot(x,col=y, pch=16)
```



### Fit training data:
```{r}
train = sample(200,100)
svmfit = svm(y~., data=dat[train,], kernel="radial", 
   gamma=1, cost=1)
par(mfrow=c(1,1))
plot(svmfit, dat[train,])
summary(svmfit)
```


### Increase cost: 
```{r}
svmfit = svm(y~., data=dat[train,], kernel="radial", gamma=1,
             cost=1e5)
par(mfrow=c(1,1))
plot(svmfit,dat[train,])
```


### Cross-validation:
```{r}
set.seed(1)
tune.out = tune(svm, y~., data=dat[train,], kernel="radial",
                ranges=list(cost=c(0.1,1,10,100,1000),
                            gamma=c(0.5,1,2,3,4)))
summary(tune.out)
```




### Test set predictions
```{r}
testData = dat[-train,"y"]
testPred = predict(tune.out$best.model, newx=dat[-train,])
table(testData, testPred)
```


# ROC Curves

### ROC plotting function 
#### This function does not work. I will need to write my own.
```{r}
# library(ROCR)

# rocplot=function(pred,truth, ...){
   # predob = prediction(pred,truth)
   # perf= performance(predob, "tpr", "fpr")
   # plot(perf,truth,...)}
```

### Get fitted values:
```{r}
# svmfit.opt = svm(y~.,data=dat[train,], kernel="radial",
   # gamma=2,cost=1,decision.values=T)
# fitted = attributes(predict(svmfit.opt,dat[train,],
   # decision.values=TRUE))$decision.values

```

### Make ROC plot:
```{r}
# windows()
# par(mfrow=c(1,2))
# rocplot(fitted,dat[train,"y"],add=T,col="red")
```



### Increase gamme to get more flexible fit, better accuracy:
```{r}
# svmfit.flex = svm(y~., data=dat[train,],kernel="radial",
   # gamma=50,cost=1, decision.values=T)
# fitted = attributes(predict(svmfit.flex,dat[train,],decision.values=T))$decision.values
# rocplot(fitted,dat[train,"y"],add=T,col="red")
```

### Compute on test set with gamma = 2:
```{r}
# fitted = attributes(predict(svmfit.opt,dat[-train,],
   # decision.values=T))$decision.values
# rocplot(fitted,dat[-train,"y"],main="Test Data")

# fitted = attributes(predict(svmfit.flex,dat[-train,],
   # decision.values=T))$decision.values
# rocplot(fitted,dat[-train,"y"], add=T, col="red")
```


# SVM with Multiple Classes

### Generate a third class:
```{r}
set.seed(1)
x = rbind(x, matrix(rnorm(50*2), ncol=2))
y = c(y, rep(0,50))
x[y==0,2] = x[y==0,2] + 2
dat = data.frame(x=x,y=as.factor(y))
par(mfrow=c(1,1))
plot(x, col=(y+1), pch=16)
```


### Fit SVM to the data:
```{r}
svmfit = svm(y~.,data=dat, kernel="radial", cost=10, gamma=1)
plot(svmfit,dat)
```




# Application to Gene Expression Data

### Examine the data:
```{r}
library(ISLR)
names(Khan)
dim(Khan$xtrain)
dim(Khan$xtest)
length(Khan$ytrain)
length(Khan$ytest)
table(Khan$ytrain)
table(Khan$ytest)
```


### Use SVM with linear kernel, since large number features relative to number of observations, implying that the flexibility of polynomial or radial kernel is not necessary.
```{r}
dat = data.frame(x=Khan$xtrain, y=as.factor(Khan$ytrain))
out = svm(y~.,data=dat, kernel="linear", cost=10)
summary(out)
table(out$fitted, dat$y)
```


### Check performance on test set:
```{r}
dat.test = data.frame(x=Khan$xtest, y=as.factor(Khan$ytest))
pred.test = predict(out, newdata=dat.test)
table(pred.test, dat.test$y)
```

Using cost = 10 results in two test set errors.




