---
title: "Chapter 8 Tree-Based Methods"
author: "CAA"
date: "July 24, 2015"
output: html_document
---

R Computer for Chapter 8 Tree-Based Methods, from the book Introduction to Statistical Learning


# Fitting classification Trees
```{r}
library(tree)
library(ISLR)

# Use the Carseats data set
attach(Carseats)

High = ifelse(Sales <= 8, "No", "Yes")

# Merge High with rest of data
Carseats = data.frame(Carseats, High)

# Fit tree using all but Sales
tree.carseats = tree(High~.-Sales, Carseats)

# Summary of model
summary(tree.carseats)
```


### Show the tree graphically:
```{r}
windows()
plot(tree.carseats)
text(tree.carseats, pretty=0)
```

### Disply the split criterion (somewhat messy):
```{r}
tree.carseats
```

### Evaluate model on training and test data sets:
```{r}
set.seed(2)
train = sample(1:nrow(Carseats),200)
Carseats.test = Carseats[-train,]
High.test = High[-train]
tree.carseats = tree(High~.-Sales, Carseats, subset=train)
tree.pred = predict(tree.carseats, Carseats.test, type="class")
table(tree.pred, High.test)
```


### Prune the tree using cross-validation:
```{r}
set.seed(3)
cv.carseats = cv.tree(tree.carseats,FUN=prune.misclass)
names(cv.carseats)
cv.carseats
```



### Plot the error rate as a function of size and k:
```{r}
par(mfrow=c(1,2))
plot(cv.carseats$size, cv.carseats$dev, type="b")
plot(cv.carseats$k, cv.carseats$dev, type="b")
```


### Apply prune.misclass() to prune tree to ninth-node tree:
```{r}
prune.carseats = prune.misclass(tree.carseats,best=9)
plot(prune.carseats)
text(prune.carseats)
```


### Evaluate performance on test data set:
```{r}
tree.pred = predict(prune.carseats, Carseats.test, type="class")
table(tree.pred, High.test)
```


### Including more nodes reduces classification accuracy:
```{r}
prune.carseats = prune.misclass(tree.carseats,best=15)
plot(prune.carseats)
text(prune.carseats, pretty=0)
tree.pred = predict(prune.carseats,Carseats.test,type="class")
table(tree.pred, High.test)
```


# Fitting Regression Trees

### Fit the tree to the Boston housing data:
```{r}
library(MASS)
set.seed(1)
train = sample(1:nrow(Boston), nrow(Boston)/2)
tree.boston = tree(medv ~.,Boston,subset=train)
summary(tree.boston)
```


### Plot the tree
```{r}
plot(tree.boston)
text(tree.boston, pretty=0)
```



### Use cross-validation to see if pruning the tree improves performance:
```{r}
cv.boston = cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type="b")
```


### We can prune the tree further if desired:
```{r}
prune.boston = prune.tree(tree.boston, best=5)
plot(prune.boston)
text(prune.boston, pretty=0)
```


### Use the unpruned tree to make predictions on the test set:
```{r}
yhat = predict(tree.boston, newdata=Boston[-train,])
boston.test = Boston[-train,"medv"]
plot(yhat, boston.test)
abline(0,1)
mean((yhat-boston.test)^2)
```



# Bagging and Random Forests

### Perform Bagging
```{r}
library(randomForest)
set.seed(1)
bag.boston = randomForest(medv~.,data=Boston,subset=train, mtry=13,importance=TRUE)
bag.boston
```


### Performance on the test set:
```{r}
yhat.bag = predict(bag.boston,newdata=Boston[-train,])
plot(yhat.bag, boston.test)
abline(0,1)
mean((yhat.bag-boston.test)^2)
```


### Change number of trees:
```{r}
bag.boston = randomForest(medv~.,data=Boston,subset=train,mtry=13,ntree=25)
yhat.bag = predict(bag.boston,newdata=Boston[-train,])
mean((yhat.bag-boston.test)^2)
```



### Grow Random Forest
```{r}
set.seed(1)
rf.boston = randomForest(medv~.,data=Boston,subset=train, mtry=6, importance=TRUE)
yhat.rf = predict(rf.boston,newdata=Boston[-train,])
mean((yhat.rf-boston.test)^2)
```


### View importance of each variable:
```{r}
importance(rf.boston)

varImpPlot(rf.boston)
```

Wealth and house size are the most important variables.


# Boosting

### Boosted regression trees
Regression problem -> distribution = "gaussian"
Use 5000 trees, and let the depth of the trees = 4
```{r}
library(gbm)
set.seed(1)
boost.boston = gbm(medv~.,data=Boston[train,], distribution = "gaussian", n.trees=5000,interaction.depth = 4)

summary(boost.boston)
```
















